{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85efbcaf",
   "metadata": {},
   "source": [
    "# 7. BERT Fine-Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76b16d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ef6db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycaret\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW\n",
    "#specify GPU\n",
    "device = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbfe152",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133f7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = pd.read_csv(\"../data/processed/balanced_fake_news_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9caea1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>original_label</th>\n",
       "      <th>dataset</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Says comprehensive immigration reform will add...</td>\n",
       "      <td>0</td>\n",
       "      <td>half-true</td>\n",
       "      <td>LIAR</td>\n",
       "      <td>says comprehensive immigration reform will add...</td>\n",
       "      <td>['say', 'comprehensive', 'immigration', 'refor...</td>\n",
       "      <td>say comprehensive immigration reform add billi...</td>\n",
       "      <td>0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ellen DeGeneres makes joke about Jennifer Anis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FakeNewsNet_Minimal</td>\n",
       "      <td>ellen degeneres makes joke about jennifer anis...</td>\n",
       "      <td>['ellen', 'degeneres', 'make', 'joke', 'jennif...</td>\n",
       "      <td>ellen degeneres make joke jennifer aniston mar...</td>\n",
       "      <td>0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When we lower tax rates, we generate more in r...</td>\n",
       "      <td>0</td>\n",
       "      <td>half-true</td>\n",
       "      <td>LIAR</td>\n",
       "      <td>when we lower tax rates we generate more in re...</td>\n",
       "      <td>['lower', 'tax', 'rate', 'generate', 'revenue'...</td>\n",
       "      <td>lower tax rate generate revenue happened reaga...</td>\n",
       "      <td>-0.2960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Karma it s a beautiful thing A massive makeshi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ISOT</td>\n",
       "      <td>karma it s a beautiful thing a massive makeshi...</td>\n",
       "      <td>['karma', 'beautiful', 'thing', 'massive', 'ma...</td>\n",
       "      <td>karma beautiful thing massive makeshift refuge...</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ellen DeGeneres' wife Portia de Rossi makes he...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FakeNewsNet_Minimal</td>\n",
       "      <td>ellen degeneres wife portia de rossi makes her...</td>\n",
       "      <td>['ellen', 'degeneres', 'wife', 'portia', 'de',...</td>\n",
       "      <td>ellen degeneres wife portia de rossi make cry ...</td>\n",
       "      <td>-0.0516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label original_label  \\\n",
       "0  Says comprehensive immigration reform will add...      0      half-true   \n",
       "1  Ellen DeGeneres makes joke about Jennifer Anis...      0              0   \n",
       "2  When we lower tax rates, we generate more in r...      0      half-true   \n",
       "3  Karma it s a beautiful thing A massive makeshi...      0              0   \n",
       "4  Ellen DeGeneres' wife Portia de Rossi makes he...      0              0   \n",
       "\n",
       "               dataset                                         clean_text  \\\n",
       "0                 LIAR  says comprehensive immigration reform will add...   \n",
       "1  FakeNewsNet_Minimal  ellen degeneres makes joke about jennifer anis...   \n",
       "2                 LIAR  when we lower tax rates we generate more in re...   \n",
       "3                 ISOT  karma it s a beautiful thing a massive makeshi...   \n",
       "4  FakeNewsNet_Minimal  ellen degeneres wife portia de rossi makes her...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['say', 'comprehensive', 'immigration', 'refor...   \n",
       "1  ['ellen', 'degeneres', 'make', 'joke', 'jennif...   \n",
       "2  ['lower', 'tax', 'rate', 'generate', 'revenue'...   \n",
       "3  ['karma', 'beautiful', 'thing', 'massive', 'ma...   \n",
       "4  ['ellen', 'degeneres', 'wife', 'portia', 'de',...   \n",
       "\n",
       "                                      processed_text  sentiment  \n",
       "0  say comprehensive immigration reform add billi...     0.2500  \n",
       "1  ellen degeneres make joke jennifer aniston mar...     0.2960  \n",
       "2  lower tax rate generate revenue happened reaga...    -0.2960  \n",
       "3  karma beautiful thing massive makeshift refuge...     0.5719  \n",
       "4  ellen degeneres wife portia de rossi make cry ...    -0.0516  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2d9562",
   "metadata": {},
   "source": [
    "# 3. Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0ff7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation-Test set split into 70 : 15 :15 ratio\n",
    "#Train -Temp split\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(bert_df['processed_text'], bert_df['label'],\n",
    "                                                                    random_state=2025,\n",
    "                                                                    test_size=0.3,\n",
    "                                                                    stratify=bert_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39e67b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text, test_text, val_labels, test_labels = train_test_split( temp_text, temp_labels,\n",
    "                                                                test_size=0.5,       # split 30% into 15% val + 15% test\n",
    "                                                                random_state=2018,\n",
    "                                                                stratify=temp_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c8711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model and tokenizer via HuggingFace Transformers\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cde6d6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Texts')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histograms of the number of words in train data processed text\n",
    "seq_len = [len(processed_text.split()) for processed_text in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 100, color = 'firebrick')\n",
    "plt.xlim(0, 200)  # Adjust as needed\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of Texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb96787",
   "metadata": {},
   "source": [
    "# 3. Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a687ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Majority of titles above have word length under 50.\n",
    "MAX_LENGHT = 50\n",
    "# Tokenize and encode sequences in the trair set\n",
    "tokens_train = tokenizer.batch_encode_plus( \n",
    "    train_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length = True ,\n",
    "    truncation = True\n",
    ")\n",
    "\n",
    "#tokenize and encode sequences in the validation set\n",
    "\n",
    "tokens_val  = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length =True,\n",
    "    truncation =True\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length =MAX_LENGHT,\n",
    "    pad_to_max_length = True,\n",
    "    truncation =True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8746f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82c52e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader structure definition \n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 32                                                                         #define a batch size         \n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)                              #wrap tensors        \n",
    "train_sampler = RandomSampler(train_data)                                               #sampler for sampling data during training\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size) # dataloader for train set \n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)       # dataloader for validation set \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f9da9",
   "metadata": {},
   "source": [
    "# 4. Freeze Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eab794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing the parameters  and defining trainable BERT structure \n",
    "\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False             # False here means gradient need not be computed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc921f53",
   "metadata": {},
   "source": [
    "# 5. Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a3bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "      super(BERT_Arch, self).__init__()\n",
    "      self.bert = bert\n",
    "      self.dropout = nn.Dropout(0.1)            # dropout layer\n",
    "      self.relu =  nn.ReLU()                    # relu activation function\n",
    "      self.fc1 = nn.Linear(768,512)             # dense layer 1\n",
    "      self.fc2 = nn.Linear(512,2)               # dense layer 2 (Output layer)\n",
    "      self.softmax = nn.LogSoftmax(dim=1)       # softmax activation function\n",
    "    def forward(self, sent_id, mask):           # define the forward pass\n",
    "      cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "                                                # pass the inputs to the model\n",
    "      x = self.fc1(cls_hs)\n",
    "      x = self.relu(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fc2(x)                           # output layer\n",
    "      x = self.softmax(x)                       # apply softmax activation\n",
    "      return x\n",
    "\n",
    "model = BERT_Arch(bert)\n",
    "# Defining the hyperparameters (optimizer, weights of the classes and the epochs)\n",
    "# Define the optimizer\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 1e-5)          # learning rate\n",
    "# Define the loss function\n",
    "cross_entropy  = nn.NLLLoss()\n",
    "# Number of training epochs\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34a414",
   "metadata": {},
   "source": [
    "# 6. Defining training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "242650b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "  model.train()\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "\n",
    "  for step,batch in enumerate(train_dataloader):                # iterate over batches\n",
    "    if step % 50 == 0 and not step == 0:                        # progress update after every 50 batches.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "    batch = [r for r in batch]                                  # push the batch to gpu\n",
    "    sent_id, mask, labels = batch\n",
    "    model.zero_grad()                                           # clear previously calculated gradients\n",
    "    preds = model(sent_id, mask)                                # get model predictions for current batch\n",
    "    loss = cross_entropy(preds, labels)                         # compute loss between actual & predicted values\n",
    "    total_loss = total_loss + loss.item()                       # add on to the total loss\n",
    "    loss.backward()                                             # backward pass to calculate the gradients\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)     # clip gradients to 1.0. It helps in preventing exploding gradient problem\n",
    "    optimizer.step()                                            # update parameters\n",
    "    preds=preds.detach().cpu().numpy()                          # model predictions are stored on GPU. So, push it to CPU\n",
    "\n",
    "  avg_loss = total_loss / len(train_dataloader)                 # compute training loss of the epoch\n",
    "                                                                # reshape predictions in form of (# samples, # classes)\n",
    "  return avg_loss                                 # returns the loss and predictions\n",
    "\n",
    "def evaluate():\n",
    "  print(\"\\nEvaluating...\")\n",
    "  model.eval()                                    # Deactivate dropout layers\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  for step,batch in enumerate(val_dataloader):    # Iterate over batches\n",
    "    if step % 50 == 0 and not step == 0:          # Progress update every 50 batches.\n",
    "                                                  # Calculate elapsed time in minutes.\n",
    "                                                  # Elapsed = format_time(time.time() - t0)\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "                                                  # Report progress\n",
    "    batch = [t for t in batch]                    # Push the batch to GPU\n",
    "    sent_id, mask, labels = batch\n",
    "    with torch.no_grad():                         # Deactivate autograd\n",
    "      preds = model(sent_id, mask)                # Model predictions\n",
    "      loss = cross_entropy(preds,labels)          # Compute the validation loss between actual and predicted values\n",
    "      total_loss = total_loss + loss.item()\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "  avg_loss = total_loss / len(val_dataloader)         # compute the validation loss of the epoch\n",
    "  return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf828e",
   "metadata": {},
   "source": [
    "# 7. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e277f3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 2\n",
      "  Batch    50  of  1,357.\n",
      "  Batch   100  of  1,357.\n",
      "  Batch   150  of  1,357.\n",
      "  Batch   200  of  1,357.\n",
      "  Batch   250  of  1,357.\n",
      "  Batch   300  of  1,357.\n",
      "  Batch   350  of  1,357.\n",
      "  Batch   400  of  1,357.\n",
      "  Batch   450  of  1,357.\n",
      "  Batch   500  of  1,357.\n",
      "  Batch   550  of  1,357.\n",
      "  Batch   600  of  1,357.\n",
      "  Batch   650  of  1,357.\n",
      "  Batch   700  of  1,357.\n",
      "  Batch   750  of  1,357.\n",
      "  Batch   800  of  1,357.\n",
      "  Batch   850  of  1,357.\n",
      "  Batch   900  of  1,357.\n",
      "  Batch   950  of  1,357.\n",
      "  Batch 1,000  of  1,357.\n",
      "  Batch 1,050  of  1,357.\n",
      "  Batch 1,100  of  1,357.\n",
      "  Batch 1,150  of  1,357.\n",
      "  Batch 1,200  of  1,357.\n",
      "  Batch 1,250  of  1,357.\n",
      "  Batch 1,300  of  1,357.\n",
      "  Batch 1,350  of  1,357.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    291.\n",
      "  Batch   100  of    291.\n",
      "  Batch   150  of    291.\n",
      "  Batch   200  of    291.\n",
      "  Batch   250  of    291.\n",
      "\n",
      "Training Loss: 0.655\n",
      "Validation Loss: 0.612\n",
      "\n",
      " Epoch 2 / 2\n",
      "  Batch    50  of  1,357.\n",
      "  Batch   100  of  1,357.\n",
      "  Batch   150  of  1,357.\n",
      "  Batch   200  of  1,357.\n",
      "  Batch   250  of  1,357.\n",
      "  Batch   300  of  1,357.\n",
      "  Batch   350  of  1,357.\n",
      "  Batch   400  of  1,357.\n",
      "  Batch   450  of  1,357.\n",
      "  Batch   500  of  1,357.\n",
      "  Batch   550  of  1,357.\n",
      "  Batch   600  of  1,357.\n",
      "  Batch   650  of  1,357.\n",
      "  Batch   700  of  1,357.\n",
      "  Batch   750  of  1,357.\n",
      "  Batch   800  of  1,357.\n",
      "  Batch   850  of  1,357.\n",
      "  Batch   900  of  1,357.\n",
      "  Batch   950  of  1,357.\n",
      "  Batch 1,000  of  1,357.\n",
      "  Batch 1,050  of  1,357.\n",
      "  Batch 1,100  of  1,357.\n",
      "  Batch 1,150  of  1,357.\n",
      "  Batch 1,200  of  1,357.\n",
      "  Batch 1,250  of  1,357.\n",
      "  Batch 1,300  of  1,357.\n",
      "  Batch 1,350  of  1,357.\n",
      "\n",
      "Evaluating...\n",
      "  Batch    50  of    291.\n",
      "  Batch   100  of    291.\n",
      "  Batch   150  of    291.\n",
      "  Batch   200  of    291.\n",
      "  Batch   250  of    291.\n",
      "\n",
      "Training Loss: 0.603\n",
      "Validation Loss: 0.554\n"
     ]
    }
   ],
   "source": [
    "# Train and predict\n",
    "best_valid_loss = float('inf')\n",
    "train_losses=[]                   # empty lists to store training and validation loss of each epoch\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    train_loss = train()                       # train model\n",
    "    valid_loss = evaluate()                    # evaluate model\n",
    "    if valid_loss < best_valid_loss:              # save the best model\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'c2_new_model_weights.pt')\n",
    "    train_losses.append(train_loss)               # append training and validation loss\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff90c0b8",
   "metadata": {},
   "source": [
    "# 8. Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1172b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weights of best model\n",
    "path = 'c2_new_model_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44773a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.70      0.74      4648\n",
      "           1       0.73      0.81      0.77      4657\n",
      "\n",
      "    accuracy                           0.76      9305\n",
      "   macro avg       0.76      0.76      0.75      9305\n",
      "weighted avg       0.76      0.76      0.75      9305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  preds = model(test_seq, test_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2d475",
   "metadata": {},
   "source": [
    "# 9. Fake News Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "203e8c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing on unseen data\n",
    "unseen_news_text = [\"Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing\",     # Fake\n",
    "                    \"WATCH: George W. Bush Calls Out Trump For Supporting White Supremacy\",               # Fake\n",
    "                    \"U.S. lawmakers question businessman at 2016 Trump Tower meeting: sources\",           # True\n",
    "                    \"Trump administration issues new rules on U.S. visa waivers\"                          # True\n",
    "                    ]\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "MAX_LENGHT = 15\n",
    "tokens_unseen = tokenizer.batch_encode_plus(\n",
    "    unseen_news_text,\n",
    "    max_length = MAX_LENGHT,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "unseen_seq = torch.tensor(tokens_unseen['input_ids'])\n",
    "unseen_mask = torch.tensor(tokens_unseen['attention_mask'])\n",
    "\n",
    "with torch.no_grad():\n",
    "  preds = model(unseen_seq, unseen_mask)\n",
    "  preds = preds.detach().cpu().numpy()\n",
    "\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9f7b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/bert_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0033682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/bert_model_tokenizer\\\\tokenizer_config.json',\n",
       " '../models/bert_model_tokenizer\\\\special_tokens_map.json',\n",
       " '../models/bert_model_tokenizer\\\\vocab.txt',\n",
       " '../models/bert_model_tokenizer\\\\added_tokens.json',\n",
       " '../models/bert_model_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../models/bert_model_tokenizer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
